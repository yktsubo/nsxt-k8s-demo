apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: nsxerrors.nsx.vmware.com
spec:
  group: nsx.vmware.com
  versions:
    - name: v1
      served: true
      storage: true
  version: v1
  scope: Cluster
  names:
    plural: nsxerrors
    singular: nsxerror
    kind: NSXError
    shortNames:
    - ne
  additionalPrinterColumns:
    - name: Messages
      type: string
      description: NSX error messages. Messages are sorted by timestamp on which the error occurs.
      JSONPath: .spec.message
    - name: ErrorObjectID
      type: string
      description: The identifier of the k8s object which has the errors.
      JSONPath: .spec.error-object-id
    - name: ErrorObjectType
      type: string
      description: The type of the k8s object which has the errors.
      JSONPath: .spec.error-object-type
    - name: ErrorObjectName
      type: string
      description: The name of the k8s object which has the errors.
      JSONPath: .spec.error-object-name
    - name: ErrorObjectNamespace
      type: string
      description: The namespace of the k8s object if it is namespaced. None by default
      JSONPath: .spec.error-object-ns
---
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: nsxlocks.nsx.vmware.com
spec:
  group: nsx.vmware.com
  versions:
    - name: v1
      served: true
      storage: true
  version: v1
  scope: Cluster
  names:
    plural: nsxlocks
    singular: nsxlock
    kind: NSXLock
    shortNames:
    - nsxlo

---

# Create Namespace for NSX owned resources
kind: Namespace
apiVersion: v1
metadata:
 name: nsx-system

---

# Create a ServiceAccount for NCP namespace
apiVersion: v1
kind: ServiceAccount
metadata:
 name: ncp-svc-account
 namespace: nsx-system
---

# Create ClusterRole for NCP
kind: ClusterRole
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: ncp-cluster-role
rules:
 - apiGroups:
   - ""
   - extensions
   - networking.k8s.io
   resources:
     - deployments
     - endpoints
     - pods
     - pods/log
     - networkpolicies
     - nodes
     - replicationcontrollers
     # Remove 'secrets' if not using Native Load Balancer.
     - secrets

   verbs:
     - get
     - watch
     - list

---

# Create ClusterRole for NCP to edit resources
kind: ClusterRole
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: ncp-patch-role
rules:
 - apiGroups:
   - ""
   - extensions
   resources:
     # NCP needs to annotate the SNAT errors on namespaces
     - namespaces
     - ingresses
     - services

   verbs:
     - get
     - watch
     - list
     - update
     - patch
 # NCP needs permission to CRUD custom resource nsxerrors
 - apiGroups:
   # The api group is specified in custom resource definition for nsxerrors
   - nsx.vmware.com
   resources:
     - nsxerrors

     - nsxlocks
   verbs:
     - create
     - get
     - list
     - patch
     - delete

     - update
 - apiGroups:
   - ""
   - extensions

   resources:
     - ingresses/status
     - services/status


   verbs:
     - replace
     - update
     - patch
---

# Bind ServiceAccount created for NCP to its ClusterRole
kind: ClusterRoleBinding
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: ncp-cluster-role-binding
roleRef:

 # Comment out the apiGroup while using OpenShift
 apiGroup: rbac.authorization.k8s.io

 kind: ClusterRole
 name: ncp-cluster-role
subjects:

 - kind: ServiceAccount
   name: ncp-svc-account
   namespace: nsx-system


---

# Bind ServiceAccount created for NCP to the patch ClusterRole
kind: ClusterRoleBinding
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: ncp-patch-role-binding
roleRef:

 apiGroup: rbac.authorization.k8s.io

 kind: ClusterRole
 name: ncp-patch-role
subjects:

 - kind: ServiceAccount
   name: ncp-svc-account
   namespace: nsx-system


---

# Create a ServiceAccount for nsx-node-agent
apiVersion: v1
kind: ServiceAccount
metadata:
 name: nsx-node-agent-svc-account
 namespace: nsx-system
---

# Create ClusterRole for nsx-node-agent
kind: ClusterRole
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: nsx-node-agent-cluster-role
rules:
 - apiGroups:
   - ""
   resources:
     - endpoints
     - services
   verbs:
     - get
     - watch
     - list

---

# Bind ServiceAccount created for nsx-node-agent to its ClusterRole
kind: ClusterRoleBinding
# Set the apiVersion to rbac.authorization.k8s.io/v1beta1 when k8s < v1.8
apiVersion: rbac.authorization.k8s.io/v1
metadata:
 name: nsx-node-agent-cluster-role-binding
roleRef:

 apiGroup: rbac.authorization.k8s.io

 kind: ClusterRole
 name: nsx-node-agent-cluster-role
subjects:

 - kind: ServiceAccount
   name: nsx-node-agent-svc-account
   namespace: nsx-system

---
# Client certificate and key used for NSX authentication
kind: Secret
metadata:
  name: nsx-secret
  namespace: nsx-system
type: kubernetes.io/tls
apiVersion: v1
data:
  # Fill in the client cert and key if using cert based auth with NSX
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURzekNDQXB1Z0F3SUJBZ0lRR2dweFJvZ01yNGZncnFkYkdVR0tjekFOQmdrcWhraUc5dzBCQVFzRkFEQmsKTVFzd0NRWURWUVFHRXdKVFJ6RUxNQWtHQTFVRUNCTUNVMGN4Q3pBSkJnTlZCQWNUQWxOSE1ROHdEUVlEVlFRSwpFd1pXVFhkaGNtVXhDekFKQmdOVkJBc1RBa05UTVIwd0d3WURWUVFERXhSWlZGTlZRazlKSUVsT1ZFVlNUVVZFClNVRlVSVEFlRncweE9UQXhNakl3TXpReU16VmFGdzB5TURBeE1qSXdNelF5TXpOYU1GNHhDekFKQmdOVkJBWVQKQWxOSE1Rc3dDUVlEVlFRSUV3SlRSekVMTUFrR0ExVUVCeE1DVTBjeER6QU5CZ05WQkFvVEJsWk5kMkZ5WlRFTApNQWtHQTFVRUN4TUNRMU14RnpBVkJnTlZCQU1URG01amNDMWpiR2xsYm5RdFkzSjBNSUlCSWpBTkJna3Foa2lHCjl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUF1bVpVZWozdElKdGY4bkxWUFNzK3ZHeWxaZENLRkxKaWhlWi8KelAzMERyV2ZBdmtBMTMyQXBMcHBxQkxaVXB0bU1EVzgrU2U4MCtKL2tMdTUzWDdYaDBWU3VsZklvdXJ2aTJIMQpraG5HVUI0ZkJnajhwaUVXZFdkK05lc2xBQnFXa09HRDU1UFg3NXFwSnh3U05jbnFXVDJIKzNDTnJTRWVXQmd5ClU4U1Raa2QveVYwNU9QamozSm5TUjZaTGptaHBkRmNBaWczNXhYNWI3UVVXb3ZFNFZia2VTMHBSN1p6R2d4ZjgKQnYzRTNicVdBMHM2S2VpY3Jpd25nZGorMEIyMmJnV2loTFBGUi9QU0FWOWpTSzBSbkkrUm1Eb3pjRzVmT2wwbAp2UTJpR3RPb3RDZGl5ZDVnZFJxbmZsWS9wWDlaSkRuZ1gxRGZvY1dvWVEvWlUxdEU1d0lEQVFBQm8yY3daVEFPCkJnTlZIUThCQWY4RUJBTUNCYUF3RXdZRFZSMGxCQXd3Q2dZSUt3WUJCUVVIQXdJd0hRWURWUjBPQkJZRUZKWnQKY2p1L3pLT1Y4NitQL0lyR0ZER2JKMHhGTUI4R0ExVWRJd1FZTUJhQUZHK1gwMXcxYi9Ja1ZFUkpKNUJ6NzhETQpheEtpTUEwR0NTcUdTSWIzRFFFQkN3VUFBNElCQVFDbUR2UVhqdjZkbFJsMDN6Mis1RWh3Ym1NM3pJbFdkUnpVCi9jdkt5MjIzUHVLbXVsVExTbGVNNThoREptQytRc1Z3MTNrZzIyY0FEVmpTTFhvS3dmQUFLRStCR0ZSeElXbGEKRGRZSUNsYSt1OWpXTDduQWE5MWxiN2k3V1pNNzdJRmR1S0hhRlBNRGJCNitDUEczS0gvaGpVRHF3Skx0N2lVbApUQnpRdTIyb1huR0daTCtZTzR2K0g4TjBFMFRZRzhNbndOR0xyS29Zd25YM0M1MnNQdFBFTmJtVEZ5VFQwVmlxCjNWa0NLQnFDa2V4bm1DSkVKdk0vczNoMmpLQTRKR2p4cXFSY0NJc0FycHFyN3VTK3NRYjhCN1p2WDJ0SGFVeWsKcU1IOThUcEdvVlNHdnc2UEdzRlN3SjU4OWxhengxdmhOZGJYWmRGRHkvM2djUmhGU2Z1MgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==
  tls.key: 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
  
---
# Certificate and key used for TLS termination in HTTPS load balancing
kind: Secret
metadata:
  name: lb-secret
  namespace: nsx-system
type: kubernetes.io/tls
apiVersion: v1
data:
  # Fill in the server cert and key for TLS termination
  tls.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQ3VENDQXRXZ0F3SUJBZ0lRZk52YlhPWmJpdWk5YmdVN0xhcUhkakFOQmdrcWhraUc5dzBCQVFzRkFEQmsKTVFzd0NRWURWUVFHRXdKVFJ6RUxNQWtHQTFVRUNCTUNVMGN4Q3pBSkJnTlZCQWNUQWxOSE1ROHdEUVlEVlFRSwpFd1pXVFhkaGNtVXhDekFKQmdOVkJBc1RBa05UTVIwd0d3WURWUVFERXhSWlZGTlZRazlKSUVsT1ZFVlNUVVZFClNVRlVSVEFlRncweE9UQXhNakl3TXpFME1qZGFGdzB5TURBeE1qSXdNekUwTWpkYU1Hc3hDekFKQmdOVkJBWVQKQWxOSE1Rc3dDUVlEVlFRSUV3SlRSekVMTUFrR0ExVUVCeE1DVTBjeER6QU5CZ05WQkFvVEJsWk5kMkZ5WlRFTApNQWtHQTFVRUN4TUNRMU14SkRBaUJnTlZCQU1URzNkcGJHUmpZWEprTG1sdVozSmxjM011WTI5eWNDNXNiMk5oCmJEQ0NBU0l3RFFZSktvWklodmNOQVFFQkJRQURnZ0VQQURDQ0FRb0NnZ0VCQU5zdFh0bi9PSUhvdFlSV1FJZUcKUFY0N0NMbVJvYmE5YmM4aVZYaWRaYk1IRlg4Z2lYZ1VtMmVBRFJaWUx0VldxUEc1NkovYVJtWmY2M0I5NUxOMgpaTTVBYlZOZnNnMXhiamE2U1BZY29IejNHWjI4RU5GNkJVbHJTbUVJemRMSkFlSWFQU1grSEpmRzZYdXdVQ3FHClo5SGROcUNJYWpYSk9kb1VHdHBTMXQ3dVNKeXpSQkVaajhJRXRvaGZidU4zVmIrdTVuRVYzUlNjMWpMT0RNWWoKVncrZzA0amh0SXc5cWZPdEFHaVY0d1hWMUo4RStEdzNGTlQ2Q3B1QUpWeW5VdGpZazZmSzZ1b2JDTksydkptNgpmaW41KzJsZEhXY2tDMXdUT3JGOHVhY1N5OFZ1K3lQQUtQZlNjZ3I1S2hpb2MvMzhRUzFMOHhtVTJLQ1cvYmprCitBc0NBd0VBQWFPQmt6Q0JrREFPQmdOVkhROEJBZjhFQkFNQ0JhQXdIUVlEVlIwbEJCWXdGQVlJS3dZQkJRVUgKQXdFR0NDc0dBUVVGQndNQ01CMEdBMVVkRGdRV0JCVE42WnlOdmNjQU9lYmZqY0NMUnE2QnZkNVFqakFmQmdOVgpIU01FR0RBV2dCUnZsOU5jTlcveUpGUkVTU2VRYysvQXpHc1NvakFmQmdOVkhSRUVHREFXZ2hRcUxtbHVaM0psCmMzTXVZMjl5Y0M1c2IyTmhiREFOQmdrcWhraUc5dzBCQVFzRkFBT0NBUUVBbW15ZG1IdUFWUSt0Yzhwbll6Z3oKOUhhbEFCT0RUQlFqdEd6ZHd5MDNIUHdKVWFNaXJXZHFodGszUHFiMjUzQVRTbVZPTEQ1THp1NTdUZUF6b3BZQgozb1pucmMvQUFYbWNyU0JlOXJHaXhDKzNKRjR6WUJIVk9LdFNzTzhmZGN6UnFwc2tzUlIwcTNQem9hamtOMG9WCmlhVVRzcDdjSGRzSW52UEY4TC9YZFExRGUwakJoT28yUnNhOVNSQ3JpenB5NitQclY3WW5wWEpJSFJLWENMb2MKNjlWLzlHZm9WTlVmQ0hXcjdoOHg0dVptekljOUY3QnpCMGJHZ0NyRFExbkZZaklFci9wNWgyeW5FMVhvWStlegpiZEVQWmZDbWVWUXZyY3NKNS9EbVVGNHJKaTBLSm9uOTlaVHJ2dXRFYWFjbWExNllneE5FREFRUzM1Sk1BeFhkCm5nPT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=
  tls.key: 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  

---
# Certificate and key used for TLS termination in HTTPS load balancing
kind: Secret
metadata:
  name: nsx-ca
  namespace: nsx-system
type: Opaque
apiVersion: v1
data:
  nsx-ca.crt: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURURENDQWpTZ0F3SUJBZ0lHQVd2Nyt2WHNNQTBHQ1NxR1NJYjNEUUVCQ3dVQU1HY3hIakFjQmdOVkJBTU0KRlc1emVHMW5jaTB3TVdFdVkyOXljQzVzYjJOaGJERVBNQTBHQTFVRUNnd0dWazEzWVhKbE1RMHdDd1lEVlFRTApEQVJPVTBKVk1Rc3dDUVlEVlFRSERBSlFRVEVMTUFrR0ExVUVDQXdDUTBFeEN6QUpCZ05WQkFZVEFsVlRNQjRYCkRURTVNRGN4TmpFNE1UQTFObG9YRFRJNU1EY3hNekU0TVRBMU5sb3daekVlTUJ3R0ExVUVBd3dWYm5ONGJXZHkKTFRBeFlTNWpiM0p3TG14dlkyRnNNUTh3RFFZRFZRUUtEQVpXVFhkaGNtVXhEVEFMQmdOVkJBc01CRTVUUWxVeApDekFKQmdOVkJBY01BbEJCTVFzd0NRWURWUVFJREFKRFFURUxNQWtHQTFVRUJoTUNWVk13Z2dFaU1BMEdDU3FHClNJYjNEUUVCQVFVQUE0SUJEd0F3Z2dFS0FvSUJBUURBSDN6MURnZ3lON1pkdUpCS1BtbTN5ejQ5N0lrVVlvN08KQklLeWxDVHpEMGVuVHRIN1ZldjF0VzhIN2xzV2p3azh0MHlUd2kwL2tRdDlySEFOVmwzaVl5Q05UUjFhYWdvaQpBVEhBT0IwUVFrOTVOZXBrWDh1OTVGTTU1TVhHaUhjdVVYeWlLeGNWOGpscUZod0J0azdaOTRkaDkxMWRyUXRXCk92Wjd0UDdmWUZsNFVJVVRLSGJwN0NnZWJrTmhtNUl1QVlRRnVVTE5ZdElzU0N1MDJVSCszQTBCdkhseXppaWgKY2pzeGVUeWZETmZlZGNYZ0t1a2Q5Slc1dTlQakV2V0Vtc21jOStiVUZWK2hKZmNhQ3ZseVRheXlndVFSZlBaYQoxanhqbzU5ZnUweHNnWnhrcEIwT1FNNnRQeVB3bC8rQTd1YVhOWXNUSjhHV3lVNWVGY2IxQWdNQkFBRXdEUVlKCktvWklodmNOQVFFTEJRQURnZ0VCQUdaUHBlR0gyY2JDZ0NaTFBUcExqRFB0V3gwWTB4cldBOG5rRDNFaG13UHIKVGRmKzJNdmV3MXZFd0d5eTVSbm5DOFpTYWwzdUNEWWNUemt0bktMcTB5ZnNFMW1nVmxpYjg4aGVBVnNoeFlvSAo4UDZFWjIxTHo1eHQveGpzWVF1QkI4ZG9wMitRK1E4cU9HSnhNb0lPYysrNjNZdzg3cUVxWXJsRVlBR09zcUZ3CllzdWNzdm1XUjlUUmFzdDNZT29sRzFvNUFlN2pnN1ZtUUMzSjUrMWFQNXk4VThtM2N5VGVwRkIxYiswSkFWaXIKV0toZVM0UHFZeEFEbnlvNlJHb1hYbnlOazNXcFZocHRZN0lIdEI5Qno2T0lhU3NrWCtxazVnY3RMWEVCaUZtdgpPVENQRzdWTDVHaHpUMzNITnRIN0ZWamthbjJJSFplS2RsWnZrbjB5V0dnPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==

---
# Yaml template for NCP Deployment
# Proper kubernetes API and NSX API parameters, and NCP Docker image
# must be specified.
# This yaml file is part of NCP 2.5.0 release.

# ConfigMap for ncp.ini
apiVersion: v1
kind: ConfigMap
metadata:
  name: nsx-ncp-config
  namespace: nsx-system
  labels:
    version: v1
data:
  ncp.ini: |
    
    [DEFAULT]

    # If set to true, the logging level will be set to DEBUG instead of the
    # default INFO level.
    debug = True

    # If set to true, log output to standard error.
    #use_stderr = True

    # If set to true, use syslog for logging.
    #use_syslog = False

    # The base directory used for relative log_file paths.
    #log_dir = <None>

    # Name of log file to send logging output to.
    #log_file = <None>

    # max MB for each compressed file. Defaults to 100 MB.
    #log_rotation_file_max_mb = 100

    # Total number of compressed backup files to store. Defaults to 5.
    #log_rotation_backup_count = 5

    # Specify the directory where nsx-python-logging is installed
    #nsx_python_logging_path = /opt/vmware/nsx-common/python

    # Specify the directory where nsx-cli is installed
    #nsx_cli_path = /opt/vmware/nsx-cli/bin/python


    [nsx_v3]


    policy_nsxapi = True

    # Path to NSX client certificate file. If specified, the nsx_api_user and
    # nsx_api_password options will be ignored. Must be specified along with
    # nsx_api_private_key_file option
    nsx_api_cert_file = /etc/nsx-ujo/nsx-cert/tls.crt

    # Path to NSX client private key file. If specified, the nsx_api_user and
    # nsx_api_password options will be ignored. Must be specified along with
    # nsx_api_cert_file option
    nsx_api_private_key_file = /etc/nsx-ujo/nsx-cert/tls.key

    # IP address of one or more NSX managers separated by commas. The IP
    # address should be of the form:
    # [<scheme>://]<ip_adress>[:<port>]
    # If
    # scheme is not provided https is used. If port is not provided port 80 is
    # used for http and port 443 for https.
    nsx_api_managers = nsxmgr-01a.corp.local

    # If True, skip fatal errors when no endpoint in the NSX management cluster
    # is available to serve a request, and retry the request instead
    cluster_unavailable_retry = False

    # Maximum number of times to retry API requests upon stale revision errors.
    #retries = 10

    # Specify one or a list of CA bundle files to use in verifying the NSX
    # Manager server certificate. This option is ignored if "insecure" is set
    # to True. If "insecure" is set to False and ca_file is unset, the system
    # root CAs will be used to verify the server certificate.
    ca_file = '/etc/nsx-ujo/nsx-ca/nsx-ca.crt'

    # If true, the NSX Manager server certificate is not verified. If false the
    # CA bundle specified via "ca_file" will be used or if unset the default
    # system root CAs will be used.
    insecure = False

    # The time in seconds before aborting a HTTP connection to a NSX manager.
    #http_timeout = 10

    # The time in seconds before aborting a HTTP read response from a NSX
    # manager.
    #http_read_timeout = 180

    # Maximum number of times to retry a HTTP connection.
    #http_retries = 3

    # Maximum concurrent connections to each NSX manager.
    #concurrent_connections = 10

    # The amount of time in seconds to wait before ensuring connectivity to the
    # NSX manager if no manager connection has been used.
    #conn_idle_timeout = 10

    # Number of times a HTTP redirect should be followed.
    #redirects = 2

    # Subnet prefix of IP block.
    #subnet_prefix = 24

    # Indicates whether distributed firewall DENY rules are logged.
    log_dropped_traffic = True


    # Option to use native load balancer or not
    use_native_loadbalancer = True


    # Option to auto scale layer 4 load balancer or not. If set to True, NCP
    # will create additional LB when necessary upon K8s Service of type LB
    # creation/update.
    l4_lb_auto_scaling = True

    # Option to use native load balancer or not when ingress class annotation
    # is missing. Only effective if use_native_loadbalancer is set to true
    default_ingress_class_nsx = True

    # Path to the default certificate file for HTTPS load balancing. Must be
    # specified along with lb_priv_key_path option
    lb_default_cert_path = /etc/nsx-ujo/lb-cert/tls.crt

    # Path to the private key file for default certificate for HTTPS load
    # balancing. Must be specified along with lb_default_cert_path option
    lb_priv_key_path = /etc/nsx-ujo/lb-cert/tls.key

    # Option to set load balancing algorithm in load balancer pool object.
    # Choices: ROUND_ROBIN LEAST_CONNECTION IP_HASH WEIGHTED_ROUND_ROBIN
    pool_algorithm = ROUND_ROBIN

    # Option to set load balancer service size. MEDIUM Edge VM (4 vCPU, 8GB)
    # only supports SMALL LB. LARGE Edge VM (8 vCPU, 16GB) only supports MEDIUM
    # and SMALL LB. Bare Metal Edge (IvyBridge, 2 socket, 128GB) supports
    # LARGE, MEDIUM and SMALL LB
    # Choices: SMALL MEDIUM LARGE
    service_size = SMALL

    # Option to set load balancer persistence option. If cookie is selected,
    # cookie persistence will be offered.If source_ip is selected, source IP
    # persistence will be offered for ingress traffic through L7 load balancer
    # Choices: <None> cookie source_ip
    #l7_persistence = <None>

    # An integer for LoadBalancer side timeout value in seconds on layer 7
    # persistence profile, if the profile exists.
    #l7_persistence_timeout = 10800

    # Option to set load balancer persistence option. If source_ip is selected,
    # source IP persistence will be offered for ingress traffic through L4 load
    # balancer
    # Choices: <None> source_ip
    #l4_persistence = <None>



    # The interval to check VIF for node. It is a workaroud for bug 2006790.
    # Old orphan LSP may not be removed on MP, so NCP will retrieve parent VIF
    # back once in a while. NCP will use the last created LSP from the list
    #vif_check_interval = 600

    # Name or UUID of the container ip blocks that will be used for creating
    # subnets. If name, it must be unique. If policy_nsxapi is enabled, it also
    # support automatically creating the IP blocks. The definition is a comma
    # separated list: CIDR,CIDR,... Mixing different formats (e.g. UUID,CIDR)
    # is not supported.
    container_ip_blocks = 'ipblock-k8s-cluster1'

    # Name or UUID of the container ip blocks that will be used for creating
    # subnets for no-SNAT projects. If specified, no-SNAT projects will use
    # these ip blocks ONLY. Otherwise they will use container_ip_blocks
    #no_snat_ip_blocks = []

    # Name or UUID of the external ip pools that will be used for allocating IP
    # addresses which will be used for translating container IPs via SNAT
    # rules. If policy_nsxapi is enabled, it also support automatically
    # creating the ip pools. The definition is a comma separated list:
    # CIDR,IP_1-IP_2,... Mixing different formats (e.g. UUID, CIDR&IP_Range) is
    # not supported.
    external_ip_pools = 'ippool-k8s-cluster1'


    # Name or UUID of the top-tier router for the container cluster network,
    # which could be either tier0 or tier1. When policy_nsxapi is enabled,
    # single_tier_topology is True and tier0_gateway is defined,
    # top_tier_router value can be empty and a tier1 gateway is automatically
    # created for the cluster
    #top_tier_router = 
    single_tier_topology = True
    tier0_gateway = T0

    # Name or UUID of the external ip pools that will be used only for
    # allocating IP addresses for Ingress controller and LB service
    #external_ip_pools_lb = []

    # Name or UUID of the NSX overlay transport zone that will be used for
    # creating logical switches for container networking. It must refer to an
    # already existing resource on NSX and every transport node where VMs
    # hosting containers are deployed must be enabled on this transport zone
    overlay_tz = acdd6be5-c503-4336-a477-cb33ab6acb3a

    # Name or UUID of the lb service that can be attached by virtual servers
    #lb_service = <None>

    # Enable X_forward_for for ingress. Available values are INSERT or REPLACE.
    # When this config is set, if x_forwarded_for is missing, LB will add
    # x_forwarded_for in the request header with value client ip. When
    # x_forwarded_for is present and its set to REPLACE, LB will replace
    # x_forwarded_for in the header to client_ip. When x_forwarded_for is
    # present and its set to INSERT, LB will append client_ip to
    # x_forwarded_for in the header. If not wanting to use x_forwarded_for,
    # remove this config
    # Choices: <None> INSERT REPLACE
    x_forwarded_for = INSERT

    # Name or UUID of the spoof guard switching profile that will be used by
    # NCP for leader election
    #election_profile = <None>

    # Name or UUID of the firewall section that will be used to create firewall
    # sections below this mark section
    #top_firewall_section_marker = <None>

    # Name or UUID of the firewall section that will be used to create firewall
    # sections above this mark section
    #bottom_firewall_section_marker = <None>

    # Replication mode of container logical switch, set SOURCE for cloud as it
    # only supports head replication mode
    # Choices: MTEP SOURCE
    #ls_replication_mode = MTEP

    # Allocate vlan ID for container interface or not. Set it to False for
    # cloud mode.
    #alloc_vlan_tag = True


    # The resource which NCP will search tag 'node_name' on, to get parent VIF
    # or transport node uuid for container LSP API context field. For HOSTVM
    # mode, it will search tag on LSP. For BM mode, it will search tag on LSP
    # then search TN. For CLOUD mode, it will search tag on VM. For WCP_WORKER
    # mode, it will search TN by hostname.
    # Choices: tag_on_lsp tag_on_tn tag_on_vm hostname_on_tn
    #search_node_tag_on = tag_on_lsp

    # Determines which kind of information to be used as VIF app_id. Defaults
    # to pod_resource_key. In WCP mode, pod_uid is used.
    # Choices: pod_resource_key pod_uid
    #vif_app_id_type = pod_resource_key

    # SNAT IP to secondary IPs mapping. In the cloud case, SNAT rules are
    # created using the PCG public or link local IPs, local IPs which will be
    # translated to PCG secondary IPs for on-prem traffic. The secondary IPs
    # might be used by administrator to configure on-prem firewall or other
    # physical network services.
    #snat_secondary_ips = {}

    # If this value is not empty, NCP will append it to nameserver list
    #dns_servers = []

    # Set this to True to enable NCP to report errors through NSXError CRD.
    #enable_nsx_err_crd = False

    # Maximum number of virtual servers allowed to create in cluster for
    # LoadBalancer type of services.
    #max_allowed_virtual_servers = 9223372036854775807

    # Edge cluster ID needed when creating Tier1 router for loadbalancer
    # service. Information could be retrieved from Tier0 router
    #edge_cluster = <None>






    [ha]


    # Time duration in seconds of mastership timeout. NCP instance will remain
    # master for this duration after elected. Note that the heartbeat period
    # plus the update timeout must not be greater than this period. This is
    # done to ensure that the master instance will either confirm liveness or
    # fail before the timeout.
    #master_timeout = 18

    # Time in seconds between heartbeats for elected leader. Once an NCP
    # instance is elected master, it will periodically confirm liveness based
    # on this value.
    #heartbeat_period = 6

    # Timeout duration in seconds for update to election resource. The default
    # value is calculated by subtracting heartbeat period from master timeout.
    # If the update request does not complete before the timeout it will be
    # aborted. Used for master heartbeats to ensure that the update finishes or
    # is aborted before the master timeout occurs.
    #update_timeout = <None>


    [coe]

    # Container orchestrator adaptor to plug in.
    adaptor = kubernetes

    # Specify cluster for adaptor.
    cluster = k8s1

    # Log level for NCP operations
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #loglevel = <None>

    # Log level for NSX API client operations
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #nsxlib_loglevel = <None>

    # Enable SNAT for all projects in this cluster
    #enable_snat = True

    # Option to enable profiling
    #profiling = False

    # The type of container host node
    # Choices: HOSTVM BAREMETAL CLOUD WCP_WORKER
    #node_type = HOSTVM

    # The time in seconds for NCP/nsx_node_agent to recover the connection to
    # NSX manager/container orchestrator adaptor/Hyperbus before exiting. If
    # the value is 0, NCP/nsx_node_agent won't exit automatically when the
    # connection check fails
    #connect_retry_timeout = 0



    [k8s]

    # Kubernetes API server IP address.
    #apiserver_host_ip = <None>

    # Kubernetes API server port.
    #apiserver_host_port = <None>

    # Full path of the Token file to use for authenticating with the k8s API
    # server.
    client_token_file = /var/run/secrets/kubernetes.io/serviceaccount/token

    # Full path of the client certificate file to use for authenticating with
    # the k8s API server. It must be specified together with
    # "client_private_key_file".
    #client_cert_file = <None>

    # Full path of the client private key file to use for authenticating with
    # the k8s API server. It must be specified together with
    # "client_cert_file".
    #client_private_key_file = <None>

    # Specify a CA bundle file to use in verifying the k8s API server
    # certificate.
    ca_file = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    # Specify whether ingress controllers are expected to be deployed in
    # hostnework mode or as regular pods externally accessed via NAT
    # Choices: hostnetwork nat
    #ingress_mode = hostnetwork

    # Log level for the kubernetes adaptor
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #loglevel = <None>

    # The default HTTP ingress port
    #http_ingress_port = 80

    # The default HTTPS ingress port
    #https_ingress_port = 443


    # Specify thread pool size to process resource events
    #resource_watcher_thread_pool_size = 1

    # User specified IP address for HTTP and HTTPS ingresses
    #http_and_https_ingress_ip = <None>

    # Set this to True to enable NCP to create segment port for VM through
    # NsxNetworkInterface CRD.
    #enable_nsx_netif_crd = False

    # Option to set the type of baseline cluster policy. ALLOW_CLUSTER creates
    # an explicit baseline policy to allow any pod to communicate any other pod
    # within the cluster. ALLOW_NAMESPACE creates an explicit baseline policy
    # to allow pods within the same namespace to communicate with each other.
    # By default, no baseline rule will be created and the cluster will assume
    # the default behavior as specified by the backend.
    # Choices: <None> allow_cluster allow_namespace
    #baseline_policy_type = <None>



    #[nsx_v3]
    # Deprecated option: tier0_router
    # Replaced by [nsx_v3] top_tier_router


---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  # VMware NSX Container Plugin
  name: nsx-ncp
  namespace: nsx-system
  labels:
    tier: nsx-networking
    component: nsx-ncp
    version: v1
spec:
  # Active-Standby is supported from NCP 2.4.0 release,
  # so replica can be more than 1 if NCP HA is enabled.
  # replica *must be* 1 if NCP HA is disabled.
  replicas: 3
  template:
    metadata:
      labels:
        tier: nsx-networking
        component: nsx-ncp
        version: v1
    spec:
      # NCP shares the host management network.
      hostNetwork: true
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      # If configured with ServiceAccount, update the ServiceAccount
      # name below.
      serviceAccountName: ncp-svc-account
      containers:
        - name: nsx-ncp
          # Docker image for NCP
          image: nsx-ncp
          imagePullPolicy: IfNotPresent
          env:
            - name: NCP_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: NCP_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - timeout 5 check_pod_liveness nsx-ncp
            initialDelaySeconds: 5
            timeoutSeconds: 5
            periodSeconds: 10
            failureThreshold: 5
          volumeMounts:
          - name: config-volume
            # NCP expects ncp.ini is present in /etc/nsx-ujo
            mountPath: /etc/nsx-ujo/ncp.ini
            subPath: ncp.ini
            readOnly: true
          # To use cert based auth, uncomment the volumeMount
          # Update ncp.ini with the mounted cert and key file paths
          - name: nsx-cert
            mountPath: /etc/nsx-ujo/nsx-cert
            readOnly: true
          # To add default LB cert, uncomment the volumeMount
          # Update ncp.ini with the mounted cert and key file paths
          - name: lb-cert
            mountPath: /etc/nsx-ujo/lb-cert
            readOnly: true
          - name: nsx-ca
            mountPath: /etc/nsx-ujo/nsx-ca
            readOnly: true
      volumes:
        - name: config-volume
          # ConfigMap nsx-ncp-config is expected to supply ncp.ini
          configMap:
            name: nsx-ncp-config
        # To use cert based auth, uncomment and update the secretName
        - name: nsx-cert
          secret:
            secretName: nsx-secret
        # To add default LB cert, uncomment and update the secretName
        - name: lb-cert
          secret:
            secretName: lb-secret
        - name: nsx-ca
          secret:
            secretName: nsx-ca

---
# Yaml template for nsx-node-agent and nsx-kube-proxy DaemonSet
# Proper kubernetes API parameters and NCP Docker image must be
# specified.
# This yaml file is part of NCP 2.5.0 release.

# ConfigMap for ncp.ini
apiVersion: v1
kind: ConfigMap
metadata:
  name: nsx-node-agent-config
  namespace: nsx-system
  labels:
    version: v1
data:
  ncp.ini: |
    
    [DEFAULT]

    # If set to true, the logging level will be set to DEBUG instead of the
    # default INFO level.
    debug = True

    # If set to true, log output to standard error.
    #use_stderr = True

    # If set to true, use syslog for logging.
    #use_syslog = False

    # The base directory used for relative log_file paths.
    #log_dir = <None>

    # Name of log file to send logging output to.
    #log_file = <None>

    # max MB for each compressed file. Defaults to 100 MB.
    #log_rotation_file_max_mb = 100

    # Total number of compressed backup files to store. Defaults to 5.
    #log_rotation_backup_count = 5

    # Specify the directory where nsx-python-logging is installed
    #nsx_python_logging_path = /opt/vmware/nsx-common/python

    # Specify the directory where nsx-cli is installed
    #nsx_cli_path = /opt/vmware/nsx-cli/bin/python


    [k8s]

    # Kubernetes API server IP address.
    #apiserver_host_ip = <None>

    # Kubernetes API server port.
    #apiserver_host_port = <None>

    # Full path of the Token file to use for authenticating with the k8s API
    # server.
    client_token_file = /var/run/secrets/kubernetes.io/serviceaccount/token

    # Full path of the client certificate file to use for authenticating with
    # the k8s API server. It must be specified together with
    # "client_private_key_file".
    #client_cert_file = <None>

    # Full path of the client private key file to use for authenticating with
    # the k8s API server. It must be specified together with
    # "client_cert_file".
    #client_private_key_file = <None>

    # Specify a CA bundle file to use in verifying the k8s API server
    # certificate.
    ca_file = /var/run/secrets/kubernetes.io/serviceaccount/ca.crt

    # Specify whether ingress controllers are expected to be deployed in
    # hostnework mode or as regular pods externally accessed via NAT
    # Choices: hostnetwork nat
    #ingress_mode = hostnetwork

    # Log level for the kubernetes adaptor
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #loglevel = <None>

    # The default HTTP ingress port
    #http_ingress_port = 80

    # The default HTTPS ingress port
    #https_ingress_port = 443


    # Specify thread pool size to process resource events
    #resource_watcher_thread_pool_size = 1

    # User specified IP address for HTTP and HTTPS ingresses
    #http_and_https_ingress_ip = <None>

    # Set this to True to enable NCP to create segment port for VM through
    # NsxNetworkInterface CRD.
    #enable_nsx_netif_crd = False

    # Option to set the type of baseline cluster policy. ALLOW_CLUSTER creates
    # an explicit baseline policy to allow any pod to communicate any other pod
    # within the cluster. ALLOW_NAMESPACE creates an explicit baseline policy
    # to allow pods within the same namespace to communicate with each other.
    # By default, no baseline rule will be created and the cluster will assume
    # the default behavior as specified by the backend.
    # Choices: <None> allow_cluster allow_namespace
    #baseline_policy_type = <None>


    [coe]

    # Container orchestrator adaptor to plug in.
    adaptor = kubernetes

    # Specify cluster for adaptor.
    cluster = k8s1

    # Log level for NCP operations
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #loglevel = <None>

    # Log level for NSX API client operations
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #nsxlib_loglevel = <None>

    # Enable SNAT for all projects in this cluster
    #enable_snat = True

    # Option to enable profiling
    #profiling = False

    # The type of container host node
    # Choices: HOSTVM BAREMETAL CLOUD WCP_WORKER
    #node_type = HOSTVM

    # The time in seconds for NCP/nsx_node_agent to recover the connection to
    # NSX manager/container orchestrator adaptor/Hyperbus before exiting. If
    # the value is 0, NCP/nsx_node_agent won't exit automatically when the
    # connection check fails
    #connect_retry_timeout = 0



    [nsx_kube_proxy]

    # The way to process service configuration, set into OVS flow or write to
    # nestdb,
    # Choices: ovs nestdb
    config_handler = ovs


    [nsx_node_agent]

    # Prefix of node /proc path to mount on nsx_node_agent DaemonSet
    #proc_mount_path_prefix = /host




    # The log level of NSX RPC library
    # Choices: NOTSET DEBUG INFO WARNING ERROR CRITICAL
    #nsxrpc_loglevel = ERROR

    # OVS bridge name
    ovs_bridge = br-int

    # The time in seconds for nsx_node_agent to wait CIF config from HyperBus
    # before returning to CNI
    #config_retry_timeout = 300

    # The time in seconds for nsx_node_agent to backoff before re-using an
    # existing cached CIF to serve CNI request. Must be less than
    # config_retry_timeout.
    #config_reuse_backoff_time = 15

    # The path for OpenvSwitch db socket
    #ovs_db_sock = unix:/var/run/openvswitch/db.sock

    # The OVS uplink OpenFlow port where to apply the NAT rules to.
    ovs_uplink_port = ens192



---
# nsx-ncp-bootstrap DaemonSet
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nsx-ncp-bootstrap
  namespace: nsx-system
  labels:
    tier: nsx-networking
    component: nsx-ncp-bootstrap
    version: v1
spec:
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        tier: nsx-networking
        component: nsx-ncp-bootstrap
        version: v1
    spec:
      hostNetwork: true
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: node.kubernetes.io/not-ready
          effect: NoSchedule
        - key: node.kubernetes.io/unreachable
          effect: NoSchedule
      hostPID: true
      # If configured with ServiceAccount, update the ServiceAccount
      # name below.
      serviceAccountName: nsx-node-agent-svc-account
      initContainers:
        - name: nsx-ncp-bootstrap
          # Docker image for NCP
          image: nsx-ncp
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["init_k8s_node"]

          securityContext:
            # privilege mode required to load apparmor on ubuntu
            privileged: true
            runAsUser: 0


          volumeMounts:
          # required to read the ovs_uplink_port
          - name: config-volume
            mountPath: /etc/nsx-ujo/ncp.ini
            subPath: ncp.ini
          # mounts to which NSX-CNI are copied BEGIN
          - name: host-etc
            mountPath: /host/etc
          - name: host-opt
            mountPath: /host/opt
          - name: host-var
            mountPath: /host/var
          # mounts to which NSX-CNI are copied END
          # mount host's OS info to identify host OS
          - name: host-os-release
            mountPath: /host/etc/os-release
          # mount host lib modules to install OVS kernel module if needed
          - name: host-modules
            mountPath: /lib/modules
          # mount openvswitch database
          - name: host-config-openvswitch
            mountPath: /etc/openvswitch
          # mount ovs runtime files
          - name: openvswitch
            mountPath: /var/run/openvswitch
          - name: dir-tmp-usr-ovs-kmod-backup
          # we move the usr kmod files to this dir temporarily before installing
          # new OVS kmod and/or backing up existing OVS kmod backup
            mountPath: /tmp/nsx_usr_ovs_kmod_backup

          # mount apparmor files to load the node-agent-apparmor
          - name: app-armor-cache
            mountPath: /var/cache/apparmor
            subPath: apparmor
          - name: apparmor-d
            mountPath: /etc/apparmor.d
          # mount linux headers for compiling OVS kmod
          - name: host-usr-src
            mountPath: /usr/src
          # mount host's deb package database to remove nsx-cni if installed
          - name: dpkg-lib
            mountPath: /host/var/lib/dpkg
          # mount for uninstalling NSX-CNI old doc
          - name: usr-share-doc
            mountPath: /usr/share/doc
          - name: snap-confine
            mountPath: /var/lib/snapd/apparmor/snap-confine


      containers:
        - name: nsx-dummy
          # This container is of no use.
          # Docker image for NCP
          image: nsx-ncp
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["/bin/bash", "-c", "while true; do sleep 5; done"]
      volumes:
        - name: config-volume
          configMap:
            name: nsx-node-agent-config
        - name: host-etc
          hostPath:
            path: /etc
        - name: host-opt
          hostPath:
            path: /opt
        - name: host-var
          hostPath:
            path: /var
        - name: host-os-release
          hostPath:
            path: /etc/os-release
        - name: host-modules
          hostPath:
            path: /lib/modules
        - name: host-config-openvswitch
          hostPath:
            path: /etc/openvswitch
        - name: openvswitch
          hostPath:
            path: /var/run/openvswitch
        - name: dir-tmp-usr-ovs-kmod-backup
          hostPath:
            path: /tmp/nsx_usr_ovs_kmod_backup

        - name: app-armor-cache
          hostPath:
            path: /var/cache/apparmor
        - name: apparmor-d
          hostPath:
            path: /etc/apparmor.d
        - name: host-usr-src
          hostPath:
            path: /usr/src
        - name: dpkg-lib
          hostPath:
            path: /var/lib/dpkg
        - name: usr-share-doc
          hostPath:
            path: /usr/share/doc
        - name: snap-confine
          hostPath:
            path: /var/lib/snapd/apparmor/snap-confine


---
# nsx-node-agent DaemonSet
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nsx-node-agent
  namespace: nsx-system
  labels:
    tier: nsx-networking
    component: nsx-node-agent
    version: v1
spec:
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:

      annotations:
        container.apparmor.security.beta.kubernetes.io/nsx-node-agent: localhost/node-agent-apparmor

      labels:
        tier: nsx-networking
        component: nsx-node-agent
        version: v1
    spec:
      hostNetwork: true
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
        - key: node.kubernetes.io/not-ready
          effect: NoSchedule
        - key: node.kubernetes.io/unreachable
          effect: NoSchedule
      # If configured with ServiceAccount, update the ServiceAccount
      # name below.
      serviceAccountName: nsx-node-agent-svc-account
      containers:

        - name: nsx-node-agent
          # Docker image for NCP
          image: nsx-ncp
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["start_node_agent"]
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - timeout 5 check_pod_liveness nsx-node-agent
            initialDelaySeconds: 5
            timeoutSeconds: 5
            periodSeconds: 10
            failureThreshold: 5
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_PTRACE
                - DAC_READ_SEARCH
          volumeMounts:
          # ncp.ini
          - name: config-volume
            mountPath: /etc/nsx-ujo/ncp.ini
            subPath: ncp.ini
            readOnly: true
          # mount openvswitch dir
          - name: openvswitch
            mountPath: /var/run/openvswitch
          # mount CNI socket path
          - name: cni-sock
            mountPath: /var/run/nsx-ujo
          # mount container namespace
          - name: netns
            mountPath: /var/run/netns
          # mount host proc
          - name: proc
            mountPath: /host/proc
            readOnly: true

        - name: nsx-kube-proxy
          # Docker image for NCP
          image: nsx-ncp
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["start_kube_proxy"]
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                - timeout 5 check_pod_liveness nsx-kube-proxy
            initialDelaySeconds: 5
            periodSeconds: 5
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_PTRACE
                - DAC_READ_SEARCH

          volumeMounts:
          # ncp.ini
          - name: config-volume
            mountPath: /etc/nsx-ujo/ncp.ini
            subPath: ncp.ini
            readOnly: true

          # mount openvswitch dir
          - name: openvswitch
            mountPath: /var/run/openvswitch


        # nsx-ovs is not needed on BAREMETAL
        - name: nsx-ovs
          # Docker image for NCP
          image: nsx-ncp
          imagePullPolicy: IfNotPresent
          # override NCP image entrypoint
          command: ["start_ovs"]
          securityContext:
            capabilities:
              add:
                - NET_ADMIN
                - SYS_ADMIN
                - SYS_NICE
                - SYS_MODULE
          livenessProbe:
            exec:
              command:
                - /bin/sh
                - -c
                # You must pass --allowOVSOnHost if you are running OVS on the
                # host before the installation. This allows livelinessProbe to
                # succeed and container won't restart frequently.
                - timeout 5 check_pod_liveness nsx-ovs
            initialDelaySeconds: 5
            periodSeconds: 5
          volumeMounts:
          # ncp.ini
          - name: config-volume
            mountPath: /etc/nsx-ujo/ncp.ini
            subPath: ncp.ini
            readOnly: true
          # mount openvswitch dir
          - name: openvswitch
            mountPath: /var/run/openvswitch
          # mount host sys dir
          - name: host-sys
            mountPath: /sys
            readOnly: true
          # mount host config openvswitch dir
          - name: host-config-openvswitch
            mountPath: /etc/openvswitch
          # mount host lib modules to insert OVS kernel module if needed
          - name: host-modules
            mountPath: /lib/modules
            readOnly: true
          # mount host's OS info to identify host OS
          - name: host-os-release
            mountPath: /host/etc/os-release
            readOnly: true


      volumes:
        - name: config-volume
          configMap:
            name: nsx-node-agent-config

        - name: openvswitch
          hostPath:
            path: /var/run/openvswitch
        - name: cni-sock
          hostPath:
            path: /var/run/nsx-ujo
        - name: netns
          hostPath:
            path: /var/run/netns
        - name: proc
          hostPath:
            path: /proc


        - name: host-sys
          hostPath:
            path: /sys
        - name: host-modules
          hostPath:
            path: /lib/modules
        - name: host-config-openvswitch
          hostPath:
            path: /etc/openvswitch
        - name: host-os-release
          hostPath:
            path: /etc/os-release



